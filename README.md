## My way of doing MNIST dataset

This time I decided that in order to learn new things I should chose something simple at the beginning, and that was a good idea.

I wanted to use few new things I learned from Stanford lectures, such as
- Use of convolutional layers
- Xawier initializer
- Batch normalization
- Ensemble learning

The training phrase is in train.ipynb notebook, and ensemble voting in ensemble.ipynb notebook. The whole process is simple so going through those notebook will be easy enough so I don't have to explain everything here.

Results:
- The best model had 98.90 % accuracy,
- The average was 98.09 %
- The ensemble accuracy won with 99.12 %

There were four models which somebody could argue that needed a bit more training, but it is not such important in educational purposes.
